---
title: "In-depth Analysis of Alibaba Cloud Panjiu AL128 Supernode AI Servers and Their Interconnect Architecture"
source: "https://www.alibabacloud.com/blog/in-depth-analysis-of-alibaba-cloud-panjiu-al128-supernode-ai-servers-and-their-interconnect-architecture_602665"
author: "Alibaba Cloud Community"
company: "Alibaba Cloud"
date: "2025"
style: "retro (realistic + vintage)"
---

# In-depth Analysis of Alibaba Cloud Panjiu AL128 Supernode AI Servers

## Article Summary

At the **Apsara Conference 2025**, Alibaba Cloud unveiled its new-generation **Panjiu AI Infra 2.0 AL128 supernode AI servers** - a fundamental shift from traditional CPU-centric to **GPU-centric architectures** designed for foundation model training and inference.

### Key Innovations

1. **Supernode Architecture**
   - 128-144 GPUs per double-wide rack
   - 350kW power supply, 500kW heat dissipation
   - 2kW liquid cooling per GPU
   - Modular design: decoupled CPU, GPU, and power nodes
   - Flexible configuration: 8, 16, or 32 GPUs per OS

2. **Three-Layer Interconnect Architecture**
   - **Layer 1 (ScaleUp)**: Single-stage switching within supernode using non-Ethernet ALink protocol (UALink, NVLink, xLink compatible). Up to 14-28 Tbit/s bandwidth per GPU with ultra-low latency.
   - **Layer 2 (ScaleOut)**: Network between supernodes with 400-800 Gbit/s per GPU via high-performance NICs.
   - **Layer 3 (DCN)**: Data center network for storage, databases, KV caches, and external communication.

3. **GPU-Centric Evolution (4 Directions)**
   - Connect peripherals via ScaleUp protocol to break PCIe limits
   - Reshape access modes to support GPU native memory semantics
   - Simplify from 3-layer to 2-layer architecture (ScaleUp + High-bandwidth DCN)
   - Apply optical interconnection (NPO, CPO) for 256+ GPU ScaleUp domains

4. **CXL Protocol for Memory**
   - Enables larger memory pools with hundreds of ns latency
   - KV cache performance improved by **4.79x**
   - Time to first token (TTFT) reduced by **82.7%**

### Core Message

> As AI model scales grow to 10+ trillion parameters, supernode servers with 128 interconnected GPUs provide the optimal balance of performance, latency, and cost for inference workloads.

---

## Visual Story

> **Note**: Images pending generation. Prompts are ready in the `prompts/` directory.

### Cover: The Supernode Revolution
*Prompt: [00-cover-supernode.md](./prompts/00-cover-supernode.md)*

A retro 1960s-70s style illustration featuring the massive AL128 server rack as an art deco industrial marvel, with Dr. Lin standing beside it for scale. Title "THE SUPERNODE REVOLUTION" in bold vintage typography.

---

### Page 1: The Growing Challenge
*Prompt: [01-page-problem.md](./prompts/01-page-problem.md)*

Visualizes the exponential growth of AI models (from billions to trillions of parameters), traditional servers struggling to keep up, and Dr. Lin introducing the supernode solution.

---

### Page 2: Meet the AL128 Supernode
*Prompt: [02-page-supernode.md](./prompts/02-page-supernode.md)*

Hero reveal of the AL128 with its 128 illuminated GPU panels, copper cooling pipes, and modular architecture. Includes blueprint-style cutaway diagrams and flexibility gauges.

---

### Page 3: ScaleUp Interconnection
*Prompt: [03-page-scaleup.md](./prompts/03-page-scaleup.md)*

Explains the single-stage switching topology, comparing ALink protocol advantages over traditional Ethernet, achieving 14-28 Tbit/s bandwidth with ultra-low latency.

---

### Page 4: Three-Layer Architecture
*Prompt: [04-page-layers.md](./prompts/04-page-layers.md)*

Visualizes the complete interconnect architecture using metaphors: ScaleOut as city highways connecting supernode skyscrapers, DCN as vintage switchboard, and a nested circles summary diagram.

---

### Page 5: Future Directions
*Prompt: [05-page-future.md](./prompts/05-page-future.md)*

Four panels covering: breaking PCIe limits, native GPU memory access, topology simplification, and optical interconnection for the light-speed future.

---

### Page 6: Conclusion
*Prompt: [06-page-conclusion.md](./prompts/06-page-conclusion.md)*

Futuristic data center vista with rows of glowing supernodes, CXL memory pools, and optical interconnects. Dr. Lin's closing statement: "128 GPUs per supernode. Ultra-low latency. The optimal balance for AI's next decade."

---

## Files Structure

```
panjiu-al128-supernode/
├── analysis.md                    # Content analysis
├── storyboard.md                 # Visual storyboard
├── panjiu-al128-supernode.md     # This summary file
├── characters/
│   └── characters.md             # Character definitions (Dr. Lin + AL128)
└── prompts/
    ├── 00-cover-supernode.md
    ├── 01-page-problem.md
    ├── 02-page-supernode.md
    ├── 03-page-scaleup.md
    ├── 04-page-layers.md
    ├── 05-page-future.md
    └── 06-page-conclusion.md
```

---

*Generated by Michi Manga (baoyu-comic skill) - Retro Style*
