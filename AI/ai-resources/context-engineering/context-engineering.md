# Context Engineering: The Definitive Research Guide (Top 50 Resources)

**Last Updated:** January 2026
**Author:** Michi

"Context Engineering" has evolved from simple prompt engineering into a sophisticated discipline. It is the art and science of optimizing the information state‚Äîsystem instructions, retrieval results (RAG), tool outputs, and conversational history‚Äîthat an LLM processes to achieve reliable, high-quality outcomes.

This guide curates the **top 50 resources** defining this field, spanning seminal industry guides from Anthropic and OpenAI, deep technical reports from Google DeepMind, and cutting-edge academic research on long-context attention.

---

## üèÜ The "Canon": Must-Read Foundations

These are the seminal texts that defined the field. If you only read 5 links, read these.

### 1. [Effective Context Engineering for AI Agents](https://www.anthropic.com/news/effective-context-engineering-for-ai-agents)
*   **Source:** Anthropic Engineering Team (Sep 2025)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The definitive manifesto from Anthropic, arguing that "Context Engineering" is the successor to "Prompt Engineering." It details strategies for managing the "attention budget" of agents, including placing long context at the top, using XML tags for structure, and the "Context Editing" technique to prune irrelevant tool outputs.
*   **Why Recommend:** It is the primary source that popularized the term "Context Engineering" for agents. Essential for understanding how to build stateful, long-running AI.
*   **Community:** [Hacker News Discussion](https://news.ycombinator.com/) (High engagement on launch)

### 2. [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
*   **Source:** Liu et al. (Stanford/Berkeley)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The seminal paper that exposed the "U-shaped" performance curve. It proved that LLMs are great at using information at the start and end of a prompt but often ignore information buried in the middle ("Lost in the Middle").
*   **Why Recommend:** Fundamental reading. It dictates *why* we structure prompts the way we do (e.g., putting queries at the end).
*   **Community:** [Hacker News Discussion](https://news.ycombinator.com/item?id=36675908)

### 3. [OpenAI Prompt Engineering Guide](https://platform.openai.com/docs/guides/prompt-engineering)
*   **Source:** OpenAI
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The gold standard documentation for interaction. While it starts with basics, it touches on advanced context tactics like "split complex tasks into simpler subtasks" and "give the model time to think" (Chain of Thought), which are context management primitives.
*   **Why Recommend:** The baseline for all other engineering.
*   **Community:** Widely cited across all developer forums.

### 4. [Gemini 1.5 Pro Technical Report (1 Million+ Token Context)](https://arxiv.org/abs/2403.05530)
*   **Source:** Google DeepMind
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** A technical deep dive into how Gemini achieved near-perfect recall in "Needle in a Haystack" tests up to 10 million tokens. It challenges the "Lost in the Middle" narrative with successful long-context architecture.
*   **Why Recommend:** Proof that architecture is changing; context engineering is shifting from "working around limits" to "managing abundance."

### 5. [Context Engineering for AI Agents: Lessons from Building Manus](https://manus.im/blog/context-engineering)
*   **Source:** Manus AI
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê¬Ω
*   **Executive Summary:** A rare "in the trenches" report from a startup building autonomous agents. It details specific tactics like "The Stable Prompt Prefix" for KV-cache optimization and using the file system as "externalized memory" to keep context windows clean.
*   **Why Recommend:** Practical, production-grade advice that goes beyond theory.
*   **Community:** Discussed on [r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/).

---

## üè≠ Industry Blueprints: How Big Tech Does It

Real-world engineering blogs on managing context at scale.

### 6. [Uber: GenAI Gateway & Prompt Engineering](https://www.uber.com/blog/generative-ai-gateway/)
*   **Source:** Uber Engineering
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Describes their centralized GenAI Gateway that standardizes context management across 100+ internal tools, using RAG to dynamically enrich prompts.

### 7. [DoorDash: Taxonomy Expansion with LLMs](https://careersatdoordash.com/blog/)
*   **Source:** DoorDash Engineering
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** How DoorDash uses constrained context windows (retrieving top 100 taxonomy concepts) to reduce noise and hallucinations in search queries. A great example of "less is more."

### 8. [Stripe: Model Context Protocol (MCP)](https://stripe.com/blog)
*   **Source:** Stripe
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** A protocol for standardizing how agents fetch documentation and API schemas. It‚Äôs "Context Engineering as a Standard" to ensure agents always have the *right* context structure.

### 9. [Airbnb: Building a Conversational Platform](https://medium.com/airbnb-engineering)
*   **Source:** Airbnb Engineering
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Focuses on their "Runtime Context Manager" which dynamically loads trip details and customer history relative to the user's intent.

### 10. [GitHub Copilot: A Developer's Guide to Prompt Engineering](https://github.blog/2023-07-17-prompt-engineering-guide-generative-ai-llms/)
*   **Source:** GitHub
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Reveals the "snippeting" pipeline‚Äîhow Copilot selects which code chunks to include in the context window from other files to maximize relevance without overflowing limits.

---

## üõ†Ô∏è Frameworks & Implementation (LangChain, LlamaIndex, Letta)

### 11. [Letta (formerly MemGPT) Whitepaper](https://arxiv.org/abs/2310.08560)
*   **Source:** Packer et al. (Letta)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Introduces "OS-level memory management" for LLMs, treating context as a finite resource with paging (swapping memory in/out of context) to create "infinite" distinct sessions.

### 12. [LangChain: Memory & Context Windows](https://python.langchain.com/docs/modules/memory/)
*   **Source:** LangChain
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** The definitive documentation on techniques like `ConversationSummaryBufferMemory`‚Äîa hybrid approach that keeps recent tokens raw but summarizes older ones to maintain long-term context.

### 13. [LlamaIndex: Long Context RAG](https://www.llamaindex.ai/blog/long-context-rag)
*   **Source:** LlamaIndex Blog
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Poses the question: "Do we need RAG if we have 1M token windows?" The answer is "Yes." It explains "Context Augmented Generation" vs RAG and how to optimize chunk sizes for large windows.

### 14. [LangGraph: Orchestrating Multi-Agent Context](https://blog.langchain.dev/langgraph-multi-agent-orchestration/)
*   **Source:** LangChain
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** How to split context across multiple agents (e.g., a "Researcher" and a "Writer") so no single agent is overwhelmed by the full state.

### 15. [Llama 3 Prompt Engineering Guide](https://llama.meta.com/docs/how-to-guides/prompting/)
*   **Source:** Meta
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Specific optimizations for Llama 3‚Äôs 128k context, emphasizing JSON formatting and specific system prompt structures that differ from GPT-4.

---

## üî¨ Academic Frontiers (2024-2025)

The cutting edge of research into efficient attention and long context.

### 16. [Needle In A Haystack: Pressure Testing LLMs](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
*   **Source:** Greg Kamradt (Community/Research)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Not just a paper, but the *standard benchmark* for context retrieval. This resource visualizes the "lost in the middle" phenomenon for every major model.

### 17. [StreamingLLM: Efficient Streaming Language Models with Attention Sinks](https://arxiv.org/abs/2309.17453)
*   **Source:** MIT/Meta
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê¬Ω
*   **Summary:** Discovered that keeping just the very first few tokens (attention sinks) allows models to stream infinite length text without crashing, even if the window is small.

### 18. [Mixture of Block Attention (MoBA)](https://arxiv.org/) (2025)
*   **Source:** Research Paper
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Proposes sparse attention blocks to handle massive contexts efficiently, reducing the quadratic cost of attention.

### 19. [LongLoRA: Efficient Fine-tuning of Long-Context LLMs](https://arxiv.org/abs/2309.12307)
*   **Source:** CUHK/MIT
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** A technique to fine-tune models for longer contexts (e.g., Llama 2 to 100k) without the massive compute usually required, using "Shifted Sparse Attention."

### 20. [RAG vs. Long Context: A Comparative Analysis](https://arxiv.org/abs/2402.09100)
*   **Source:** DeepMind/Google
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** A direct scientific comparison. Conclusion: Long context is better for holistic understanding, but RAG is better for locating specific facts in massive corpora.


### 21. [LongSkywork: efficient Context Extension](https://arxiv.org/abs/2406.00605)
*   **Source:** Skywork AI (2024)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** A comprehensive "training recipe" for upgrading standard LLMs to long-context models (up to 200k tokens). It introduces "Chunk Interleaving" for data and a specialized long-context fine-tuning stage that requires surprisingly few steps.
*   **Why Recommend:** Practical playbook for engineers training their own models; shows you don't need to retrain from scratch.

### 22. [TokenSelect: Efficient Long-Context Inference](https://arxiv.org/abs/2402.10044)
*   **Source:** Microsoft Research
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** A technique to reduce KV cache size by identifying "critical tokens" via soft voting. It proves that not all tokens in context are equal and allows for 2-3x speedups in inference by essentially "forgetting" the useless parts of the context.
*   **Why Recommend:** Critical for optimizing the *cost* of long context.

### 23. [Core Context Aware Attention](https://arxiv.org/abs/2405.15016)
*   **Source:** Research Paper
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Addresses the redundancy in long contexts. It compresses global context into "core tokens" while preserving local details, effectively performing context compression on the fly to keep attention linear rather than quadratic.

### 24. [Ring Attention: Near-Infinite Context](https://arxiv.org/abs/2310.01889)
*   **Source:** Liu, Zaharia et al. (UC Berkeley)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The architectural breakthrough that allows context to scale across *multiple devices*. By passing blocks of keys/values in a ring between GPUs, it breaks the memory wall of a single card, enabling million-token contexts.
*   **Why Recommend:** The infrastructure foundation for models like Gemini 1.5.

### 25. [FlashAttention-2](https://arxiv.org/abs/2307.08691)
*   **Source:** Tri Dao (Stanford)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The hardware optimization that makes long context *possible*. By optimizing how GPUs read/write to memory (IO-awareness), it speeds up attention constraints by 2-4x.
*   **Why Recommend:** Without this, "Context Engineering" for long inputs would be prohibitively slow.

### 26. [YaRN: Efficient Context Window Extension](https://arxiv.org/abs/2309.00071)
*   **Source:** Nous Research
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê¬Ω
*   **Executive Summary:** "Yet another RoPE extensioN." A method to extend the context window of Llama models (e.g., to 128k) by modifying the positional embeddings (RoPE) and using a temperature scaling trick, without massive retraining.
*   **Why Recommend:** The community standard for extending open-source models.

### 27. [Leave No Context Behind (Infini-attention)](https://arxiv.org/abs/2404.07143)
*   **Source:** Google
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Introduces "Compressive Memory" into the attention block, allowing the model to carry an "infinite" recurrent state alongside its standard attention window. A hybrid between Transformers and RNNs.
*   **Why Recommend:** Likely the secret sauce behind Gemini's massive 2M+ window.

### 28. [Self-Refine: Iterative Refinement](https://arxiv.org/abs/2303.17651)
*   **Source:** Carnegie Mellon / Google
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Proves that LLMs can act as their own critics. By feeding the model's output back into its context as a draft ("Feedback" -> "Refine"), performance improves significantly without new data.
*   **Why Recommend:** A core context pattern: treating the *history* of the conversation as a scratchpad.

### 29. [RL for Prompt Optimization (RLPrompt, etc.)](https://arxiv.org/abs/2205.12548)
*   **Source:** Research Community
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Applying Reinforcement Learning (like PPO or TRPO concepts) to the *prompt* space itself. Instead of optimizing model weights, you optimize the input tokens to maximize a reward function.
*   **Why Recommend:** The future of "automated" context engineering.

### 30. [Chain of Density Prompting](https://arxiv.org/abs/2309.04269)
*   **Source:** Salesforce Research / MIT
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê¬Ω
*   **Executive Summary:** A specific prompting technique where you ask the model to summarize a text, then summarize *that* summary while adding *more* entities (increasing density).
*   **Why Recommend:** The best known method for creating "dense" context summaries that fit into limited windows.

---

## üß≠ Community Masterpieces & Collections

Guides written by the best engineers in the open source community.

### 31. [Awesome Context Engineering (GitHub)](https://github.com/meirtz/awesome-context-engineering)
*   **Source:** Community (Meirtz)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** The most comprehensive GitHub list tracking papers, tools, and blog posts specifically on this topic.

### 32. [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering)
*   **Source:** Brex Engineering
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Production-grade tips from a fintech unicorn. Their section on "defensive context" (preventing injection) is unique.

### 33. [AgentDock: LLM Context Window Optimizer](https://agentdock.ai)
*   **Source:** AgentDock
*   **Rating:** ‚≠ê‚≠ê‚≠ê
*   **Summary:** A practical tool and guide for chunking content intelligently to fit overlapping windows.

### 34. [Lilian Weng: Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
*   **Source:** OpenAI Head of Safety (Personal Blog)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** One of the most technically accurate and cited blog posts on the mechanics of prompting and context.

### 35. [Gorilla LLM: API Call Context](https://gorilla.cs.berkeley.edu/)
*   **Source:** Berkeley
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Summary:** Focuses on context engineering specifically for *tool use* and API calling, optimizing how API docs are presented to the model.

### 36. [Prompt Engineering Guide (dair.ai)](https://www.promptingguide.ai/)
*   **Source:** DAIR.AI
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The "Wikipedia" of prompting. It catalogs every major technique (CoT, ToT, ReAct) with papers and examples. A living document that tracks the state of the art.
*   **Why Recommend:** The best starting point for definitions and standard patterns.

### 37. [AutoGPT: The Evolution from Vector DBs](https://github.com/Significant-Gravitas/AutoGPT)
*   **Source:** Significant Gravitas
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** AutoGPT's journey is a lesson in context management. They famously moved *away* from complex Vector Database integration for core memory, finding it "overkill," favoring simpler JSON/file-based context for reliability.
*   **Why Recommend:** A real-world counter-narrative to "put everything in a vector DB."

### 38. [BabyAGI: Task Prioritization Loops](https://github.com/yoheinakajima/babyagi)
*   **Source:** Yohei Nakajima
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The original "Agent" loop. It manages context by splitting it into "Task List" and "Execution Result." A dedicated "Prioritization Agent" constantly reorders future tasks based on past context, preventing the "drift" common in long chains.
*   **Why Recommend:** Defines the primitive loop of context-aware agents.

### 39. [Voyager: Skill Libraries as Context](https://voyager.minedojo.org/)
*   **Source:** NVIDIA / Caltech
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** An embodied agent in Minecraft that writes its own code. Its breakthrough is the "Skill Library"‚Äîit stores successful code snippets (skills) and retrieves them as context for future similar problems.
*   **Why Recommend:** Demonstrates "executable context"‚Äîstoring logic, not just text.

### 40. [Generative Agents: Memory Streams](https://github.com/joonspk-research/generative_agents)
*   **Source:** Stanford / Google (Park et al.)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The famous "Sims" paper. It introduces the **Memory Stream** architecture, scoring memories by Recency, Importance, and Relevance to determine what enters the context window. It also adds a "Reflection" step to synthesize high-level thoughts.
*   **Why Recommend:** The definitive architecture for "human-like" memory in agents.

### 41. [ReAct: Synergizing Reasoning and Acting](https://arxiv.org/abs/2210.03629)
*   **Source:** Princeton / Google
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The paper that defined modern agents. It proved that interleaving "Thought" (Reasoning) and "Action" (Context from Tools) performs better than doing either in isolation.
*   **Why Recommend:** The fundamental pattern for all agentic context loops (LangChain, etc.).

### 42. [Chain-of-Note: Improving Retrieval](https://arxiv.org/abs/2311.09210)
*   **Source:** Tencent AI
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Enhances RAG by asking the model to take "sequential reading notes" on retrieved documents before answering. This allows the model to filter out irrelevant or noisy retrievals that might poison the context.
*   **Why Recommend:** A robust defense against "bad retrieval" in RAG systems.

### 43. [System 2 Attention](https://arxiv.org/abs/2311.11829)
*   **Source:** Meta AI
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê¬Ω
*   **Executive Summary:** A technique where the LLM is asked to purely *rewrite* the prompt to remove irrelevant information before processing it. It separates the "cleaning" of context from the "reasoning," significantly improving factuality.
*   **Why Recommend:** Treating context cleaning as a distinct "System 2" step.

### 44. [LLMLingua: Prompt Compression](https://llmlingua.com/)
*   **Source:** Microsoft
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** A method to compress prompts by up to 20x with minimal performance loss. It uses a smaller model to identify and remove "low-perplexity" (predictable) tokens, keeping only the "surprising" (high-value) context.
*   **Why Recommend:** Essential for saving money and latency on massive prompts.

### 45. [DSPy: Programming‚ÄîNot Prompting](https://dspy.ai/)
*   **Source:** Stanford NLP
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** A radical shift that abstracts prompts away entirely. You define "Signatures" (Input/Output types) and "Modules," and DSPy "compiles" the optimal prompt/context for you, optimizing it like a neural network.
*   **Why Recommend:** It treats context engineering as a *compiler* problem, not a writing problem.

### 46. [NexusFlow: Tool-Use Fine-Tuning](https://nexusflow.ai/)
*   **Source:** NexusFlow
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Focuses on models (like NexusRaven) specifically fine-tuned for function calling. They demonstrate that "context engineering" isn't just about the prompt, but aligning the model to understand API schemas deeply.
*   **Why Recommend:** Best for enterprise agents that need to use 20+ tools reliably.

### 47. [CodeLlama: Infilling](https://ai.meta.com/blog/code-llama-large-language-model-coding/)
*   **Source:** Meta AI
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Popularized "Infilling" (Fill-In-The-Middle), a context strategy where the model looks at both the prefix (code before) and suffix (code after) to generate the middle. A specialized form of bidirectional context.

### 48. [AlphaCode 2: Search & Reranking](https://deepmind.google/technologies/alphacode/)
*   **Source:** Google DeepMind
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** Generates millions of diverse code samples and uses a scoring model to select the best one. It treats "context" as a massive search space of potential solutions, filtering down to the correct one.
*   **Why Recommend:** SOTA in competitive programming; shows the power of "generate many, filter one."

### 49. [Harrison Chase on Context Engineering](https://twitter.com/hwchase17)
*   **Source:** Social Media / Blog
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The CEO of LangChain coined the "Context-Aware Reasoning Application" terminology. His heuristic: "The capability of an agent is limited by the context it can effectively retrieve and reason over."
*   **Why Recommend:** Practical wisdom from building the most popular agent framework.

### 50. [Karpathy: State of GPT (Context as RAM)](https://karpathy.ai/stateofgpt.pdf)
*   **Source:** Andrej Karpathy (Build 2023)
*   **Rating:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
*   **Executive Summary:** The definitive mental model: **"Context Window is the working memory (RAM) of the OS."** He explains that we must engineer this RAM usage carefully because transformers have "perfect memory" inside the window but zero memory outside it.
*   **Why Recommend:** The single best analogy for understanding why this entire field matters.

